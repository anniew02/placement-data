library(broom)
library(MASS)
library(cowplot)
library(tidyverse)
library(caret)     
library(pROC)
library(knitr)
install.packages("kableExtra")
library(kableExtra)
install.packages("dplyr")
library(dplyr)
library(httr)
library(utils)
library(readr)
install.packages("corrplot")
library(corrplot)
install.packages("car")
library(car)

data_raw <- "https://raw.githubusercontent.com/MichelleHon/Placement-Status-inference/refs/heads/main/placementdata.csv"
data <- read.csv(data_raw)
head(data)            # Displays first few rows of the data columns
nrow(data)            # Displays the total number of rows in the entire dataset
colSums(is.na(data))  # Displays the number of empty fields in each column of the dataset


placement <- data.frame(
  VariableName = c(
    "StudentID",
    "CGPA", 
    "Internships", 
    "Projects", 
    "Workshops.Certifications",
    "AptitudeTestScore", 
    "SoftSkillRating", 
    "ExtracurricularActivities",
    "PlacementTraining",
    "SSC_Marks",
    "HSC_Marks", 
    "PlacementStatus (Response Variable)"
  ),
  Type = c(
    "Continuous",
    "Continuous", 
    "Continuous", 
    "Continuous", 
    "Continuous", 
    "Continuous",
    "Continuous", 
    "Binary", 
    "Binary",
    "Continuous",
    "Continuous",  
    "Binary"
  ),
  Description = c(
    "Unique Identification for each canditdate",
    "Overall grade point average.", 
    "Number of internships completed.", 
    "Number of projects completed.",
    "Count of completed workshops or certifications.",
    "Score on a quantitative/logical aptitude test.",
    "Rating of communication and interpersonal skills.",
    "Involvement in non-academic activities (Yes/No).",
    "Indicates whether the student received placement training (Yes/No).",
    "Senior Secondary marks.",
    "Higher Secondary marks.",
    "Final placement outcome (Placed/Not Placed)."
  )
)
kable(placement)



#As mentioned earlier we drop the StudentID variable since it does not provide meaningful information to help predict.
data_model <- data %>% select(-StudentID)      

# Convert known categorical variables to factors.
# Here we assume that PlacementStatus, ExtracurricularActivities, PlacementTraining 
data_model$PlacementStatus           <- as.factor(data_model$PlacementStatus)
data_model$ExtracurricularActivities <- as.factor(data_model$ExtracurricularActivities)
data_model$PlacementTraining         <- as.factor(data_model$PlacementTraining)

# Variable Descriptions (all other variables are retained):
# - CGPA, AptitudeTestScore, SSC_Marks, HSC_Marks, Internships, Projects, WorkshopsCertifications, SoftSkillRating: continuous indicators.
# - ExtracurricularActivities, PlacementTraining: binary indicators.
# - PlacementStatus: binary response variable.

full_model <- glm(PlacementStatus ~ ., data = data_model, family = binomial)
summary(full_model)




# Summary statistics for the entire dataset
summary(data_model)

# HERE WE CHECK IF THE DISTRIBUTON OF EACH COLUMN OF DATA IS APPROPRIATE OR NOT (ANY OUTLIERS ???)

# Histograms of numeric variables to check their distributions in order to make sure assumptions are valid.
# We first select the numeric variables (CGPA, AptitudeTestScore, SSC_Marks, HSC_Marks, Internships, Projects, Workshops.Certifications, SoftSkillsRating)
data_model %>%
  select(CGPA, AptitudeTestScore, SSC_Marks, HSC_Marks, Internships, Projects, Workshops.Certifications, SoftSkillsRating) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Value)) +
    geom_histogram(bins = 30, fill = "steelblue", color = "black") +
    facet_wrap(~ Variable, scales = "free") +
    theme_minimal() +
    labs(title = "Histograms of Numeric Variables", x = "Value", y = "Count")

# Boxplots to detect potential outliers in numeric variables (same variables as above).
data_model %>%
  select(CGPA, AptitudeTestScore, SSC_Marks, HSC_Marks, Internships, Projects, Workshops.Certifications, SoftSkillsRating) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Variable, y = Value)) +
    geom_boxplot(fill = "orange") +
    theme_minimal() +
    labs(title = "Boxplots of Numeric Variables", x = "Variable", y = "Value")

# Now that thats done, we will look at Bar plots for the categorical variables in the dataset.
data_model %>%
  select(PlacementStatus, ExtracurricularActivities, PlacementTraining) %>%
  pivot_longer(cols = everything(), names_to = "Category", values_to = "Value") %>%
  ggplot(aes(x = Value, fill = Value)) +
    geom_bar() +
    facet_wrap(~ Category, scales = "free") +
    theme_minimal() +
    labs(title = "Bar Plots of Categorical Variables", x = "Category", y = "Count")



# HERE WE CHECK FOR THE CORRELATION AND MULTICOLLINEARITY BETWEEN VARIABLES

# Selecting all numeric variables for correlation analysis.
numeric_vars <- data_model %>%
  select(CGPA, AptitudeTestScore, SSC_Marks, HSC_Marks, Internships, Projects, Workshops.Certifications, SoftSkillsRating)

# Computing and printing the correlation matrix these numeric variables.
cor_matrix <- cor(numeric_vars, use = "complete.obs")
print(cor_matrix)

# Visualize the correlation matrix using the corrplot.
corrplot(cor_matrix, method = "number", type = "upper", tl.cex = 0.8)

# Check for multicollinearity by calculating Variance Inflation Factors (VIF) using the car.
vif_values <- vif(full_model)
print(vif_values)




residualPlots(full_model)



###################################
Most numeric variables (for example, AptitudeTestScore, CGPA, HSC_Marks) have relatively continuous distributions, though some like Internships, Projects are clustered around a few integer values (thats solely due to the fact that there are only 3 and 4 values in each respectively). The boxplots reveal minor outliers (in Internships), but nothing extreme. Overall, the distributions suggest that logistic regression is reasonable. Slight skewness in integer-based variables might prompt us to consider transformations or interactions if linearity in the log-odds appears violated, this will be explored further using residual plots.
###################################

###################################
We can see from the VIF values that none of the variables exceed 10 (or even 5 for the matter), and also none of them are highly correlated (above 0.7, the maximum observed is 0.57) hence it does not seem necessary to omit any of the covariates from the analysis.
###################################

###################################
The partial residual plots suggest that most predictors have a roughly linear relationships with the log-odds of placements, although there is mild curvature in certain variables. A few plots also show potential outliers or heavier tails during EDA, indicating that transformations or polynomial terms might slightly improve model fit. Overall, the linearity assumption does not appear severely violated, but these small deviations need further investigation if we want to refine the modelâ€™s accuracy which will be taken care of in the final submission after learning the new performance metric. The significance tests also confirm that many of these predictors have a meaningful association with the log-odds of being placed. 
###################################

